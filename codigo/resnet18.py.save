import torch
if not torch.cuda.is_available():
	print("unable to run on GPU")
	exit(-1)
import torchvision #computer vision dataset module
import torchvision.models as models
from torchvision import datasets,transforms
from torch import nn

import numpy as np
import os


def lr_scheduler(epoch):
	if epoch < 150:
		return 0.1
	elif epoch < 250:
		return 0.01
	elif epoch < 350:
		return 0.001

if __name__ == '__main__':
	ain=transforms.Compose([transforms.RandomCrop(32, padding=4),
	                   transforms.RandomHorizontalFlip(),
	                   transforms.ToTensor(),
	                   transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]) #transforms are different for train and test

	cifar10_transforms_test=transforms.Compose([transforms.ToTensor(),
	                   transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])


	#cargamos el dataset CIFAR10
	workers = (int)(os.popen('nproc').read())
	cifar10_train=datasets.CIFAR10('/tmp/',train=True,download=True,transform=cifar10_transforms_train)
	cifar10_test=datasets.CIFAR10('/tmp/',train=False,download=False,transform=cifar10_transforms_test)

	#creamos los dataloaders para iterar el conjunto de datos
	train_loader = torch.utils.data.DataLoader(cifar10_train,batch_size=100,shuffle=True,num_workers=workers)
	test_loader = torch.utils.data.DataLoader(cifar10_test,batch_size=100,shuffle=False,num_workers=workers)

	loss = nn.CrossEntropyLoss()
	seeds = []
	losses = []
	testErrors = []
	modelos = []
	#generamos 5 semillas aleatorias y creamos un modelo para cada semilla
	for i in range(5):
		seeds.append(np.random.randint(150))
		modelos.append(models.resnet18(False))
	#para cada semilla realizamos el entrenamiento y clasificacion del modelo
	for seed,modelo in zip(seeds, modelos):
		resnet18 = modelo
		resnet18.cuda()
		torch.cuda.manual_seed(seed)
		scheduler=lr_scheduler
		for e in range(350):
			ce_test,MC,ce=[0.0]*3
			optimizer=torch.optim.SGD(resnet18.parameters(),lr=scheduler(e),momentum=0.9)
			for x,t in train_loader:
				x,t=x.cuda(),t.cuda()
				resnet18.train()
				o=resnet18.forward(x)
				cost=loss(o,t)
				cost.backward()
				optimizer.step()
				optimizer.zero_grad()
				ce+=cost.data

			with torch.no_grad():
				for x,t in test_loader:
					x,t=x.cuda(),t.cuda()
					resnet18.eval()
					test_pred=resnet18.forward(x)
					index=torch.argmax(test_pred,1) #compute maximum
					MC+=(index!=t).sum().float() #accumulate MC error

			print("Epoch {} cross entropy {:.5f} and Test error {:.3f}".format(e,ce/500.,100*MC/10000.))

		losses.append(ce/500.)
		testErrors.append(100*MC/10000.)
		print("--------------------------------------------------------")
		print("--------------------------------------------------------")

	avgCE = 0.0
	avgTestError = 0.0
	print(">>>> Resultados: ")
	for i in range(len(seeds)):
		print("\tModelo {}: cross entropy {:.5f} and Test error {:.3f}".format(i+1, losses[i], testErrors[i]))
		avgCE+=losses[i]/len(losses)
		avgTestError+=testErrors[i]/len(testErrors)

	print("Valores medios finales: cross entropy {:.5f} and Test error {:.3f}".format(avgCE, avgTestError))
